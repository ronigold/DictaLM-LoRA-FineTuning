{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b831c63-ceea-4bb4-896b-1970c3ebcd49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, GPT2LMHeadModel, GPT2Config\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "from ollama_interact import *\n",
    "\n",
    "def parse_qa_from_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Use regex to split based on \"Q \\d+:\" to find questions and their answers\n",
    "    chunks = re.split(r'Q \\d+:\\n', text)\n",
    "\n",
    "    # Remove the first empty chunk if it exists\n",
    "    if chunks[0] == '':\n",
    "        chunks.pop(0)\n",
    "\n",
    "    qa_data = []\n",
    "\n",
    "    # Define the regex patterns for instruction (question), input, and output (answer)\n",
    "    question_pattern = re.compile(r'instruction_hebrew: (.*?)\\n')\n",
    "    input_pattern = re.compile(r'input_hebrew: (.*?)\\n')\n",
    "    answer_pattern = re.compile(r'output_hebrew: (.*)', re.DOTALL)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        question_match = question_pattern.search(chunk)\n",
    "        input_match = input_pattern.search(chunk)\n",
    "        answer_match = answer_pattern.search(chunk)\n",
    "\n",
    "        if question_match and answer_match:\n",
    "            # Clean the answer text to replace newline followed by number and dot to maintain the format\n",
    "            cleaned_answer = re.sub(r'\\n\\d+\\.', '', answer_match.group(1))\n",
    "            qa_data.append({\n",
    "                'question': question_match.group(1),\n",
    "                'input': input_match.group(1),\n",
    "                'answer': cleaned_answer.strip()  # Strip to remove leading/trailing whitespace\n",
    "            })\n",
    "\n",
    "    return qa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629b4ccb-d4c2-4863-b8f0-c5646a067a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_name = \"vicgalle/alpaca-gpt4\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    data_name, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d981d0fe-725c-45c3-8e33-5601874d87fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 52002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccbe8666-839b-4302-ae84-c534435127c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the file you uploaded\n",
    "file_path = 'alpaca_gpt4_hebrew.txt'\n",
    "qa_list = parse_qa_from_text(file_path)\n",
    "\n",
    "# Convert to JSON and print the JSON string\n",
    "json_data = json.dumps(qa_list, indent=4, ensure_ascii=False)\n",
    "json_file_path = 'converted_data.json'\n",
    "\n",
    "with open(json_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "668850b0-e382-47e1-986e-78f82fc7e16a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Analyze the given text for its tone.',\n",
       " 'input': 'The world has been greatly impacted by the COVID-19 pandemic and it has drastically changed our lives.',\n",
       " 'output': 'The tone of the text is serious and somber. The use of terms such as \"greatly impacted,\" \"drastically changed,\" and \"pandemic\" suggest the seriousness and gravity of the situation, and convey a sense of heaviness and concern.',\n",
       " 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnalyze the given text for its tone.\\n\\n### Input:\\nThe world has been greatly impacted by the COVID-19 pandemic and it has drastically changed our lives.\\n\\n### Response:\\nThe tone of the text is serious and somber. The use of terms such as \"greatly impacted,\" \"drastically changed,\" and \"pandemic\" suggest the seriousness and gravity of the situation, and convey a sense of heaviness and concern.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "503a439c-d27d-4570-ae1d-e03b342105a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'בדקו את הטקסט שניתן לכם כדי לקבוע את מצב הרוח שלו.',\n",
       " 'input': 'העולם עבר טלטלה משמעותית בעקבות מגפת הקורונה, והיא שינתה את חיינו באופן דרמטי.',\n",
       " 'answer': 'אופי הטקסט הוא רציני וקודר. שימוש בביטויים כמו \"השפעה משמעותית מאוד\", \"שינוי באופן דרסטי\", ו\"מגיפה\" מרמז על החומרה והכבדות של המצב, ומביע תחושה של חרדה וכובד.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_list[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67525fe8-dd5a-49c2-968b-700dd5977220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21601"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78f93455-bc96-433e-86ea-c1ab5d078a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "new_data = []\n",
    "\n",
    "for item in qa_list:\n",
    "    new_data.append({\n",
    "        'instruction': item['question'],\n",
    "        'input': item['input'],\n",
    "        'output': item['answer'],\n",
    "    })\n",
    "\n",
    "for item in dataset['train']:\n",
    "    if random.random() < 0.1:\n",
    "        new_data.append({\n",
    "            'instruction': item['instruction'],\n",
    "            'input': item['input'],\n",
    "            'output': item['output'],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174d86fb-58b3-41e8-a653-7ed7c4a2ce5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate translation requests\n",
    "for he_item, en_item in zip(qa_list, dataset['train']):\n",
    "    content_to_translate = random.choice(['question_he', 'answer_he'])\n",
    "    direction = random.choice(['he_to_en', 'en_to_he'])\n",
    "    if content_to_translate == 'question_he':\n",
    "        if direction == 'he_to_en':\n",
    "            sentence = he_item['question'] \n",
    "            translation = en_item['instruction']\n",
    "        else:\n",
    "            sentence = en_item['instruction']\n",
    "            translation = he_item['question']\n",
    "    else:\n",
    "        if direction == 'he_to_en':\n",
    "            sentence = he_item['answer'] \n",
    "            translation = en_item['output']\n",
    "        else:\n",
    "            sentence = en_item['output']\n",
    "            translation = he_item['answer']\n",
    "        \n",
    "    prompt_1 = 'אנא תרגם את המשפט הבא לאנגלית, המשפט בעברית:' if direction == 'he_to_en' else 'Please translate the following sentence to Hebrew:'\n",
    "    prompt_2 = 'The translation of the sentence in English is:' if direction == 'he_to_en' else 'התרגום של המשפט בעברית הוא:'\n",
    "\n",
    "    new_data.append({\n",
    "        'instruction': f'{prompt_1} {sentence}',\n",
    "        'input': '',\n",
    "        'output': f'{prompt_2} {translation}',\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04f1cab7-b9cc-42ce-b1cd-0dc020fcc4ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48336"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cb818ce-40cc-46e0-b346-5c0386c7eecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hebrew_Questions_and_Answers.txt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'Hebrew_Questions_and_Answers.txt'\n",
    "# Format the data into a string suitable for saving\n",
    "formatted_data = \"\".join(f\"<s>[INST]{entry['input']} {entry['instruction']}[/INST] {entry['output']} </s>\" for entry in new_data)\n",
    "\n",
    "# Writing data to file\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(formatted_data)\n",
    "\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071ea05-c9f8-4f64-9747-01f7bc027105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Open the original text file and read the content\n",
    "with open('Hebrew_Questions_and_Answers.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Find all segments enclosed by <s>[INST] and </s>\n",
    "segments = re.findall(r'(<s>\\[INST\\].*?</s>)', content, re.DOTALL)\n",
    "\n",
    "# Replace newlines within each segment with \\n\n",
    "processed_segments = [segment.replace('\\n', '\\\\n') for segment in segments]\n",
    "\n",
    "# Open a new file to write the processed content\n",
    "with open('output.txt', 'w', encoding='utf-8') as file:\n",
    "    for segment in processed_segments:\n",
    "        file.write(segment + '\\n')  # Write each processed segment followed by a newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bab3d4c-50a4-445e-94db-804cea094549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to JSON and print the JSON string\n",
    "json_data = json.dumps(new_data, indent=4, ensure_ascii=False)\n",
    "json_file_path = 'all_data.json'\n",
    "\n",
    "with open(json_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
